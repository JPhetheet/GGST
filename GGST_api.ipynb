{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AGl4cpljG71X",
        "Li6ghiwejrz7",
        "1-fKeIPRkovf",
        "iZ3T8kbaHU1B",
        "WSsWtTAPj72A",
        "7GGt79BN2dtb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Instructions:**\n",
        "*This is a Google Colab Notebook, shared via Github. You may perform operations within this notebook and download your files once you're done. Your changes will not be saved if you refresh this page. You can save a copy of this notebook by clicking on the File tab, then save a copy.*\n",
        " \n",
        "* **To run each cell, press the play button on the left. Occasionally, the cell will require additional input after pressing the play button.**\n",
        "* If an error appears, double check that all input options are correct.\n",
        "\n",
        "For more detailed documentation, please visit http://hydroinf.groups.et.byu.net/servir-wa/ggst/api.php"
      ],
      "metadata": {
        "id": "5YuGGPqrLDra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Packages and Select your Tethys Portal"
      ],
      "metadata": {
        "id": "AGl4cpljG71X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Run this cell to install dependencies**\n",
        "#@markdown Be patient, this cell may take a few moments to load\n",
        "# Install dependencies\n",
        "%%capture\n",
        "!pip install xarray \n",
        "!pip install geopandas\n",
        "!pip install basemap"
      ],
      "metadata": {
        "id": "Jih2ZEumLRnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Run this cell to load necessary packages**\n",
        "# Load packages\n",
        "%%capture\n",
        "import requests\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from io import BytesIO, StringIO\n",
        "import xarray\n",
        "import json\n",
        "import numpy\n",
        "import pandas\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from netCDF4 import Dataset as NetCDFFile \n",
        "from PIL import Image\n",
        "import glob\n",
        "from os import path\n",
        "from mpl_toolkits.basemap import Basemap\n",
        "import shutil"
      ],
      "metadata": {
        "id": "2EXciYkvLXIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Run this cell to declare the portal**\n",
        "#@markdown Select your preferred portal from the list or type in your own\n",
        "portal = \"https://tethyswa.servirglobal.net\" #@param [\"https://tethys.byu.edu\",\"https://tethys-staging.byu.edu\" ,\"https://tethyswa.servirglobal.net\"] {allow-input: true}\n",
        "# use  https://tethyswa.servirglobal.net tethyswa.servirglobal.net for the Portal hosted by the SERVIR WA\n",
        "# use https://tethys-staging.byu.edu for the Portal hosted by BYU staging\n",
        "# use https://tethys.byu.edu for the Portal hosted by BYU. We encourage to use any portal but staging."
      ],
      "metadata": {
        "id": "JufwfGONOAgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function 1: getStorageOptions"
      ],
      "metadata": {
        "id": "Li6ghiwejrz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Run this cell to view storage options**\n",
        "# View available storage types\n",
        "storage_options = requests.get(portal+ \"/apps/ggst/api/getStorageOptions/\")\n",
        "storage_options.json()"
      ],
      "metadata": {
        "id": "3WhydVA1cWSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function 2: getPointValues"
      ],
      "metadata": {
        "id": "1-fKeIPRkovf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Set desired inputs for getPointValues function and run this cell**\n",
        "Latitude =  33#@param {type:\"number\"}\n",
        "Lat = str(Latitude)\n",
        "Longitude = -80 #@param {type:\"number\"}\n",
        "Lon = str(Longitude)\n",
        "F2_Storage_Option = \"gw\" #@param [\"gw\", \"sm\", \"sw\", \"swe\", \"tws\", \"canopy\", \"grace\"]\n",
        "\n",
        "#Run getPointValues function with your inputs\n",
        "pointvalues = requests.get(portal + \"/apps/ggst/api/getPointValues/?latitude=\"+Lat+\"&longitude=\"+Lon+\"&storage_type=\"+F2_Storage_Option)"
      ],
      "metadata": {
        "id": "YYCdBAoi6lhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Run this cell to convert the request to a dataframe and view the timeseries table**\n",
        "# Get the json object from the request\n",
        "pointvalues_json = pointvalues.json()\n",
        "# Create a dataframe from the JSON for plotting\n",
        "point_ts = (pandas.DataFrame(columns=[\"date\", \"ts\"], data=pointvalues_json[\"values\"])\n",
        "            .merge(pandas.DataFrame(columns=[\"date\", \"error_min\", \"error_max\"], data=pointvalues_json[\"error_range\"]), on=\"date\"))\n",
        "point_ts[\"date\"] =  pandas.to_datetime(point_ts.date*1000000)\n",
        "point_ts"
      ],
      "metadata": {
        "id": "f2l0cWXE6oMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Run this cell to plot the dataframe**\n",
        "# Plot the dataframe with error range\n",
        "fig, ax = plt.subplots(1, 1, figsize=(25,5))\n",
        "ax.plot(point_ts.date, point_ts.ts)\n",
        "ax.fill_between(point_ts.date, point_ts.error_min, point_ts.error_max, alpha=0.35)\n",
        "ax.set_title(F2_Storage_Option+' Anomaly Timeseries at Point ' +Lat+', '+Lon)"
      ],
      "metadata": {
        "id": "OGeQIta64oPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requesting Info for Regional Functions 3 and 4\n"
      ],
      "metadata": {
        "id": "iZ3T8kbaHU1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ###**Input api token in the quotations below and run cell**\n",
        "# Get API Token from User Settings on the Tethys Portal. \n",
        "# Note: You have to be logged in to access User Settings\n",
        "#@markdown Make sure your API token is generated from the same portal you declared at the beginning of this notebook. *A token registered under a different portal will result in errors*.\n",
        "api_token = \"\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "XV-AveiZmKy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ###**Input region name then run cell and follow prompts**\n",
        "#@markdown Select a region name, it must be python friendly so no spaces please.\n",
        "region_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown After you hit run, this cell will prompt you to upload a file below. This file must be a zipfile contain all four parts of a shapefile (shp, shx, dbf, and prj).\n",
        "#upload zipped shapefile\n",
        "uploaded = files.upload()\n",
        "#set up files for API request\n",
        "region_files = {'shapefile': (\"response.zip\", uploaded[\"\".join(uploaded)],'application/zip')}"
      ],
      "metadata": {
        "id": "ns00hVyyMRZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function 3: getRegionTimeseries"
      ],
      "metadata": {
        "id": "WSsWtTAPj72A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Set inputs for getRegionTimeseries then run the cell to run the function**\n",
        "F3_Storage_Option = \"gw\" #@param [\"gw\", \"sm\", \"sw\", \"swe\", \"tws\", \"canopy\", \"grace\"]\n",
        "# Initialize timeseries request. API Token is passed in the headers for authorization.\n",
        "# Name and Storage Type parameters are passed in the data dictionary object\n",
        "storage_type = F3_Storage_Option\n",
        "data_obj = {\"name\": region_name,\n",
        "            \"storage_type\": storage_type}\n",
        "region_timeseries_request = requests.post(portal + \"/apps/ggst/api/getRegionTimeseries/\",\n",
        "                                          headers={\"Authorization\": f\"Token {api_token}\"},\n",
        "                                          data = data_obj, \n",
        "                                          files= region_files)"
      ],
      "metadata": {
        "id": "It4uDytKBwII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Run this cell to convert the request to a dataframe view the timeseries table**\n",
        "\n",
        "# Get the json object from the request\n",
        "region_ts_json = region_timeseries_request.json()\n",
        "# Create a dataframe from the JSON for plotting\n",
        "region_ts = (pandas.DataFrame(columns=[\"date\", \"ts\"], data=region_ts_json[\"values\"])\n",
        "            .merge(pandas.DataFrame(columns=[\"date\", \"error_min\", \"error_max\"], data=region_ts_json[\"error_range\"]), on=\"date\"))\n",
        "region_ts[\"date\"] =  pandas.to_datetime(region_ts.date)\n",
        "region_ts\n"
      ],
      "metadata": {
        "id": "PidCoIqbgVRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Plot the dataframe with error range**\n",
        "fig, ax = plt.subplots(1, 1, figsize=(25,5))\n",
        "ax.plot(region_ts.date, region_ts.ts)\n",
        "ax.fill_between(region_ts.date, region_ts.error_min, region_ts.error_max, alpha=0.35)\n",
        "ax.set_title(region_name + ' Average ' + F3_Storage_Option +' Storage Anomaly')"
      ],
      "metadata": {
        "id": "judHDiAGHNT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function 4: getRegionZipfile"
      ],
      "metadata": {
        "id": "7GGt79BN2dtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Run this cell to read the zipfile and extract its content**\n",
        "# Declare region name\n",
        "data_obj = {\"name\": region_name}\n",
        "subset_region_request = requests.post(portal + \"/apps/ggst/api/subsetRegionZipfile/\",\n",
        "                                      headers={\"Authorization\": f\"Token {api_token}\"},\n",
        "                                      data = data_obj, files=region_files)\n",
        "#Save the request response and extract the files\n",
        "z = ZipFile(BytesIO(subset_region_request.content))\n",
        "z.extractall()"
      ],
      "metadata": {
        "id": "xqUYzazJakNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting RegionZipfile"
      ],
      "metadata": {
        "id": "BOmtoT0k3fnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Run this cell to initiate a function for processing raw data**\n",
        "# Helper function to calculate regional timeseries for raw data\n",
        "def get_regional_ts(region, storage_type, grace_dir):\n",
        "    graph_json = {}\n",
        "    ts_plot = []\n",
        "    ts_plot_int = []\n",
        "    error_range = []\n",
        "    nc_file = os.path.join(grace_dir, f\"{region}_{storage_type}.nc\")\n",
        "    ds = xarray.open_dataset(nc_file)\n",
        "    region_area = json.load(open(os.path.join(grace_dir, \"area.json\"), \"r\"))[\n",
        "        \"area\"\n",
        "    ]\n",
        "    lwe_da = ds.lwe_thickness.mean([\"lat\", \"lon\"])\n",
        "    error_da = ds.uncertainty.mean([\"lat\", \"lon\"])\n",
        "\n",
        "    init_value = lwe_da.values[0]\n",
        "    for x, y in zip(lwe_da, error_da):\n",
        "        value = x.values\n",
        "        error_bar = y.values\n",
        "        utc_time = x.time.values\n",
        "        difference_data_value = (value - init_value) * 0.00000075 * region_area\n",
        "        ts_plot.append([utc_time, round(float(value), 3)])\n",
        "        error_range.append(\n",
        "            [\n",
        "                utc_time,\n",
        "                round(float(value - error_bar), 3),\n",
        "                round(float(value + error_bar), 3),\n",
        "            ]\n",
        "        )\n",
        "        ts_plot_int.append([utc_time, round(float(difference_data_value), 3)])\n",
        "\n",
        "    graph_json[\"values\"] = ts_plot\n",
        "    graph_json[\"depletion_values\"] = ts_plot_int\n",
        "    graph_json[\"error_range\"] = error_range\n",
        "    graph_json[\"area\"] = region_area\n",
        "    df = (pandas.DataFrame(ts_plot, columns=[\"time\", \"value\"])\n",
        "          .merge(pandas.DataFrame(ts_plot_int, columns=[\"time\", \"depletion\"]))\n",
        "          .merge(pandas.DataFrame(error_range, columns=[\"time\", \"min_bar\", \"max_bar\"])))\n",
        "    df[\"time\"] = pandas.to_datetime(df[\"time\"])\n",
        "    return graph_json, df"
      ],
      "metadata": {
        "id": "4lZl78SmcT-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Select which storage option you wish to plot and run cell to get Timeseries**\n",
        "\n",
        "F4_Storage_Option = \"gw\" #@param [\"gw\", \"sm\", \"sw\", \"swe\", \"tws\", \"canopy\", \"grace\"]\n",
        "gw_json, gw_df = get_regional_ts(region_name, F4_Storage_Option, region_name)\n",
        "gw_df"
      ],
      "metadata": {
        "id": "mBP32VVpceIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Run this cell to plot Groundwater Anomaly Timeseries and Depletion Curve**\n",
        "# Plotting two timeseries\n",
        "fig, ax = plt.subplots(2, 1, figsize=(25, 10))\n",
        "ax[0].plot(gw_df.time, gw_df.value)\n",
        "ax[0].fill_between(gw_df.time, gw_df.min_bar, gw_df.max_bar, alpha=0.35)\n",
        "ax[0].set_title(region_name+ ' Regional Average ' + F4_Storage_Option+\" Storage Anomaly Timeseries\")\n",
        "ax[1].plot(gw_df.time, gw_df.depletion)\n",
        "ax[1].set_title(region_name+ ' ' +F4_Storage_Option+\" Depletion Curve\")\n",
        "plt.subplots_adjust(wspace=0.4, hspace=0.5)"
      ],
      "metadata": {
        "id": "T2y2J9Ps09Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing RegionZipfile\n"
      ],
      "metadata": {
        "id": "RI6fAvIFzcIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Select which NetCDF you wish to view and run this cell to return it's characteristics**\n",
        "NETCDF_abv = \"gw\" #@param [\"gw\", \"sm\", \"sw\", \"swe\", \"tws\", \"canopy\", \"grace\", \"025sw\", \"025sm\", \"025gw\", \"025canopy\", \"025swe\", \"025tws\"]\n",
        "nc = xarray.open_dataset('/content/'+region_name+'/'+region_name+'_'+NETCDF_abv+'.nc')\n",
        "lat,lon,time,var = nc.variables['lat'][:],nc.variables['lon'][:],nc.variables['time'][:], nc.variables['lwe_thickness'][:]\n",
        "begin = numpy.datetime64(nc.coords['time'].values[0], 'D')\n",
        "end = numpy.datetime64(nc.coords['time'].values[len(nc['time'][:])-1], 'D')\n",
        "print('Begins on: ' + str(begin))\n",
        "print('Ends on: ' + str(end))\n",
        "print('')\n",
        "print(NETCDF_abv+ ' min: ' +str(numpy.amin(var)))\n",
        "print('')\n",
        "print(NETCDF_abv+ ' max: ' +str(numpy.amax(var)))"
      ],
      "metadata": {
        "id": "y1yWuxF8TrnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Change Parameters to Customize your Images and GIF animation**\n",
        "#@markdown Choose a start and date within the dataset range (our datasets are monthly so the exact day is not important):\n",
        "startdate = \"2008-04-01\" #@param {type:\"date\"}\n",
        "enddate = \"2012-01-01\" #@param {type:\"date\"}\n",
        "#@markdown Cosmetic Options:\n",
        "cmap_colorscheme = \"Spectral\" #@param [\"jet\", \"viridis\", \"Spectral\", \"rainbow\"] {allow-input: true}\n",
        "frame_duration = 300 #@param {type:\"slider\", min:100, max:1000, step:50}\n",
        "EsriBasemaps = \"World_Topo_Map\" #@param [\"NatGeo_World_Map\", \"USA_Topo_Maps\", \"World_Imagery\", \"World_Physical_Map\", \"World_Shaded_Relief\", \"World_Street_Map\", \"World_Terrain_Base\", \"World_Topo_Map\"]\n",
        "Frame_Cushion = 1 #@param {type:\"slider\", min:0, max:15, step:0.5}\n",
        "\n",
        "#Set up the time index for your GIF\n",
        "starttime = numpy.datetime64(startdate, 'M')\n",
        "endtime = numpy.datetime64(enddate, 'M')\n",
        "startindex, endindex = -1, -1\n",
        "for i in range(len(nc['time'][:])-1):\n",
        "  check = numpy.datetime64(nc.coords['time'].values[i], 'M')\n",
        "  if starttime < check and startindex == -1:\n",
        "    startindex = i-1\n",
        "  if endtime < check and endindex == -1: \n",
        "    endindex = i+1"
      ],
      "metadata": {
        "id": "a2DqLMMYA9CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Run this cell to create individual timestamped images**\n",
        "if path.exists('/content/'+region_name+NETCDF_abv+'_imgs') == False:\n",
        "  os.mkdir('/content/'+region_name+NETCDF_abv+'_imgs')\n",
        "#setting up the figure and map\n",
        "plt.figure(figsize=(12,6))\n",
        "lons,lats=numpy.meshgrid(lon,lat)\n",
        "topbound = numpy.amax(lats)+0.5\n",
        "bottombound = numpy.amin(lats)-0.5\n",
        "rightbound= numpy.amax(lons)+0.5\n",
        "leftbound = numpy.amin(lons)-0.5\n",
        "m = Basemap(projection='cyl', llcrnrlon=leftbound-Frame_Cushion, llcrnrlat=bottombound-Frame_Cushion, urcrnrlon=rightbound+Frame_Cushion, urcrnrlat=topbound+Frame_Cushion, resolution='i', epsg=4326)\n",
        "#m.bluemarble(alpha=0.8)\n",
        "m.arcgisimage(server='http://server.arcgisonline.com/ArcGIS', service=EsriBasemaps, xpixels = 400, dpi = 69,verbose = True)\n",
        "m.drawcoastlines(linewidth = 1.0,color='k')\n",
        "m.drawstates(linewidth = 1.0,color='k')\n",
        "m.drawcountries(linewidth = 1.0,color='k')\n",
        "m.drawparallels(numpy.arange(bottombound-Frame_Cushion,topbound+Frame_Cushion,1),labels=[1,0,0,1], fontsize=8)\n",
        "m.drawmeridians(numpy.arange(leftbound-Frame_Cushion,rightbound+Frame_Cushion,1),labels=[1,0,0,1], rotation=45, fontsize=8)\n",
        "\n",
        "#creating the first image so that we can create the color scheme and labels\n",
        "im = plt.imshow(var[startindex,:,:], cmap=cmap_colorscheme,extent=(leftbound, rightbound, topbound, bottombound),vmin=numpy.amin(var),vmax=numpy.amax(var))\n",
        "plt.title(region_name+' ' +NETCDF_abv+' Anomaly '+str(nc.coords['time'].values[startindex])[:10], pad = 15), plt.xlabel('Longitude',labelpad=40), plt.ylabel('Latitude',labelpad=40)\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label(\"Liquid Water Equivalent (cm)\", labelpad = 20)\n",
        "plt.savefig(f\"/content/{region_name+NETCDF_abv}_imgs/'{region_name+NETCDF_abv+str(nc.coords['time'].values[startindex])[:10]}.jpg\")\n",
        "\n",
        "#looping through the remaining images\n",
        "for i in range(startindex+1,endindex):\n",
        "  im = plt.imshow(var[i,:,:], cmap=cmap_colorscheme,extent=(leftbound, rightbound, topbound, bottombound),vmin=numpy.amin(var),vmax=numpy.amax(var))\n",
        "  plt.title(region_name+' ' +NETCDF_abv+' Anomaly '+str(nc.coords['time'].values[i])[:10], pad = 15)\n",
        "  plt.savefig(f\"/content/{region_name+NETCDF_abv}_imgs/'{region_name+NETCDF_abv+str(nc.coords['time'].values[i])[:10]}.jpg\")\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "i-i1_YDM8x9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Run this cell to stitch images together into a GIF animation**\n",
        "frames = []\n",
        "imgs=glob.glob(f\"/content/{region_name+NETCDF_abv}_imgs/*.jpg\")\n",
        "imgs.sort()\n",
        "for i in imgs:\n",
        "    new_frame = Image.open(i)\n",
        "    frames.append(new_frame)\n",
        "    frames[0].save(region_name+NETCDF_abv+'_animation.gif', format='GIF', append_images=frames[1:],save_all=True, duration = frame_duration, loop=0)"
      ],
      "metadata": {
        "id": "ARO9ioRC_ZnO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}